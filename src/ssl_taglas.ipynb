{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrahma56/anaconda3/envs/cs519/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "sys.path.append('/home/mrahma56/cs519/SSL_LLM_Node_Classification')\n",
    "from TAGLAS import get_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEED = 1234\n",
    "# SEED = 4567\n",
    "SEED = 7890\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# DATASET = 'cora'\n",
    "DATASET = 'wikics'\n",
    "LLM_ID = \"Llama-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mrahma56/cs519/SSL_LLM_Node_Classification/saved_embeddings/wikics_NV-Embed-v2.pt\n"
     ]
    }
   ],
   "source": [
    "dataset_key_dict = {\n",
    "    'cora': 'cora_node',\n",
    "    'wikics': 'wikics'\n",
    "}\n",
    "\n",
    "\n",
    "root_dir = \"/home/mrahma56/cs519/SSL_LLM_Node_Classification/\"\n",
    "taglas_dir = root_dir + \"TAGLAS/\"\n",
    "llm_gen_dir = root_dir + \"llm_gen_data/\"\n",
    "saved_model_dir = root_dir + \"saved_models/\"\n",
    "saved_embedding_dir = root_dir + \"saved_embeddings/\"\n",
    "embedding_model = \"nvidia/NV-Embed-v2\"\n",
    "embedding_path = saved_embedding_dir + f\"{dataset_key_dict[DATASET]}_{embedding_model.split('/')[-1]}.pt\"\n",
    "print(embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taglas_dataset(dataset_key=\"cora_node\", unlabel_ratio=None,embedding_path=None, print_info=True):\n",
    "    # Load the dataset from TAGLAS\n",
    "    dataset = get_dataset(dataset_key, root=taglas_dir)\n",
    "    data = dataset._data\n",
    "\n",
    "    # Set train, validation, and test masks based on the dataset key\n",
    "    if dataset_key == \"cora_node\":\n",
    "        data.train_lb_mask = dataset.side_data['node_split']['train'][0].clone()\n",
    "        data.val_mask = dataset.side_data['node_split']['val'][0].clone()\n",
    "        data.test_mask = dataset.side_data['node_split']['test'][0].clone()\n",
    "    elif dataset_key == \"wikics\":\n",
    "        data.train_lb_mask = dataset.side_data['node_split']['train'][:, 0].clone()\n",
    "        data.val_mask = dataset.side_data['node_split']['val'][:, 0].clone()\n",
    "        data.test_mask = dataset.side_data['node_split']['test'].clone()\n",
    "    \n",
    "    # Map labels and features\n",
    "    data.y = data.label_map\n",
    "    data.x_text = data.x\n",
    "    data.x = data.x_original\n",
    "    if embedding_path is not None and os.path.exists(embedding_path):\n",
    "        print(\"Loading embedding from: \", embedding_path)\n",
    "        data.x = torch.load(embedding_path)\n",
    "    \n",
    "    \n",
    "    # Add num_classes to data\n",
    "    data.num_classes = dataset.num_classes\n",
    "    data.train_ulb_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    \n",
    "\n",
    "    if unlabel_ratio is not None and unlabel_ratio > 0:\n",
    "        # Get indices of training nodes from the labeled training mask\n",
    "        train_indices = data.train_lb_mask.nonzero(as_tuple=True)[0]\n",
    "        \n",
    "        # Get labels of training nodes\n",
    "        train_labels = data.y[train_indices]\n",
    "        \n",
    "        # Initialize the mask for unlabeled training nodes\n",
    "        data.train_ulb_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "        \n",
    "        class_label_counts = []  # Store labeled/unlabeled counts per class\n",
    "\n",
    "        for cls in range(data.num_classes):\n",
    "            # Get indices of training nodes belonging to the current class\n",
    "            class_indices = train_indices[train_labels == cls]\n",
    "            num_class_nodes = len(class_indices)\n",
    "            \n",
    "            # Calculate the number of nodes to unlabel (70%) and label (30%) for this class\n",
    "            nodes_to_unlabel = int(unlabel_ratio * num_class_nodes)\n",
    "            nodes_to_label = num_class_nodes - nodes_to_unlabel\n",
    "            \n",
    "            # Randomly select nodes to unlabel for this class\n",
    "            unlabeled_indices = class_indices[torch.randperm(num_class_nodes)[:nodes_to_unlabel]]\n",
    "            \n",
    "            # Update the unlabeled mask\n",
    "            data.train_ulb_mask[unlabeled_indices] = True\n",
    "            \n",
    "            # Count labeled and unlabeled samples for the class\n",
    "            class_label_counts.append((cls, nodes_to_label, nodes_to_unlabel))\n",
    "        \n",
    "        # Update the labeled training mask\n",
    "        data.train_lb_mask[data.train_ulb_mask] = False\n",
    "\n",
    "    if print_info and unlabel_ratio:\n",
    "        # Print the information about the unlabeled and labeled nodes\n",
    "        print(f\"Unlabeled ratio: {unlabel_ratio}\")\n",
    "        print(f\"Labeled training nodes: {data.train_lb_mask.sum().item()}\")\n",
    "        print(f\"Unlabeled training nodes: {data.train_ulb_mask.sum().item()}\")\n",
    "        \n",
    "        # Print class-wise statistics\n",
    "        print(\"\\nClass-wise labeled and unlabeled counts:\")\n",
    "        for cls, num_labeled, num_unlabeled in class_label_counts:\n",
    "            print(f\"Class {cls}: Labeled = {num_labeled}, Unlabeled = {num_unlabeled}\")\n",
    "    \n",
    "    # Retain only the required keys in the data object\n",
    "    required_keys = [\n",
    "        'x', 'y', 'train_lb_mask', 'train_ulb_mask', \n",
    "        'val_mask', 'test_mask', 'num_classes', \n",
    "        'num_features', 'x_text', 'edge_index', 'edge_attr'\n",
    "    ]\n",
    "    for k in list(data.keys()):\n",
    "        if k not in required_keys:\n",
    "            data.pop(k)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to relabel low-confidence samples\n",
    "def relabel_samples(low_conf_indices, data):\n",
    "    num_classes = data.num_classes\n",
    "    gold_label_prob = 0.7  # Probability of assigning the gold label\n",
    "\n",
    "    # Generate random probabilities for each low-confidence sample\n",
    "    random_probs = torch.rand(len(low_conf_indices))\n",
    "\n",
    "    # Initialize new labels with random class labels\n",
    "    random_labels = torch.randint(0, num_classes, (len(low_conf_indices),))\n",
    "\n",
    "    # Assign gold labels with probability `gold_label_prob`\n",
    "    new_labels = torch.where(\n",
    "        random_probs < gold_label_prob,\n",
    "        data.y[low_conf_indices],  # Gold labels\n",
    "        random_labels  # Random class labels\n",
    "    )\n",
    "\n",
    "    return new_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to relabel low-confidence samples\n",
    "def llm_label_samples(low_conf_indices, data, dataset=\"cora\", llm_id=\"Llama-3\"):\n",
    "    # print(f\"Dataset: {dataset}\")\n",
    "    num_classes = data.num_classes\n",
    "    llm_gen_file = os.path.join(llm_gen_dir, f\"{dataset}_{llm_id}.tsv\")\n",
    "    df = pd.read_csv(llm_gen_file, sep='\\t')\n",
    "    y_gen = torch.tensor(df['llm_label'].values)\n",
    "    y_gen = torch.where((y_gen >= 0) & (y_gen < num_classes) , y_gen, torch.zeros_like(y_gen))\n",
    "    return y_gen[low_conf_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_semi_supervised(model, data, optimizer, x, y, alpha=0.1, th=0.5, llm_label=False):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = model(x, data.edge_index)\n",
    "    out_prob = F.softmax(logits, dim=1)\n",
    "\n",
    "    labeled_loss = torch.tensor(0.0, device=x.device)\n",
    "    consistency_loss = torch.tensor(0.0, device=x.device)\n",
    "    \n",
    "    num_low_conf_samples = 0\n",
    "\n",
    "    # Labeled loss\n",
    "    if data.train_lb_mask.sum() > 0:\n",
    "        labeled_loss = F.cross_entropy(logits[data.train_lb_mask], y[data.train_lb_mask])\n",
    "\n",
    "    # Consistency loss and relabeling\n",
    "    if data.train_ulb_mask.sum() > 0 and llm_label:\n",
    "        pseudo_labels = out_prob[data.train_ulb_mask].argmax(dim=1)\n",
    "        confidence_scores = out_prob[data.train_ulb_mask].max(dim=1).values\n",
    "        confident_mask = confidence_scores > th\n",
    "        low_conf_mask = ~confident_mask\n",
    "        \n",
    "        # print(f\"Confidence Scores: {confidence_scores}\")\n",
    "        # print(f\"Confident Mask: {confident_mask}\")\n",
    "        # print(f\"Low Confidence Mask: {low_conf_mask}\")\n",
    "\n",
    "        confident_indices = data.train_ulb_mask.nonzero(as_tuple=True)[0][confident_mask]\n",
    "        low_conf_indices = data.train_ulb_mask.nonzero(as_tuple=True)[0][low_conf_mask]\n",
    "        num_low_conf_samples = len(low_conf_indices)\n",
    "        # print(f\"Num low Confident Indices: {num_low_conf_samples}\")\n",
    "\n",
    "        # Consistency loss for high-confidence samples\n",
    "        if len(confident_indices) > 0:\n",
    "            consistency_loss = F.cross_entropy(logits[confident_indices], pseudo_labels[confident_mask])\n",
    "\n",
    "        # Relabel low-confidence samples\n",
    "        if len(low_conf_indices) > 0:\n",
    "            # new_labels = relabel_samples(low_conf_indices, data)\n",
    "            new_labels = llm_label_samples(low_conf_indices, data, dataset=DATASET, llm_id=LLM_ID)\n",
    "\n",
    "            # Create new masks instead of modifying in-place\n",
    "            new_train_lb_mask = data.train_lb_mask.clone()\n",
    "            new_train_ulb_mask = data.train_ulb_mask.clone()\n",
    "\n",
    "            # Update the masks with relabeled samples\n",
    "            new_train_lb_mask[low_conf_indices] = True\n",
    "            new_train_ulb_mask[low_conf_indices] = False\n",
    "\n",
    "            # Assign the new labels and masks\n",
    "            y[low_conf_indices] = new_labels\n",
    "            data.train_lb_mask = new_train_lb_mask\n",
    "            data.train_ulb_mask = new_train_ulb_mask\n",
    "\n",
    "    # Total loss\n",
    "    loss = labeled_loss + alpha * consistency_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return num_low_conf_samples, loss.item(), labeled_loss.item(), consistency_loss.item(), y, data.train_lb_mask, data.train_ulb_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation step\n",
    "def validation_step(model, data, x, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x, data.edge_index)\n",
    "        loss = F.cross_entropy(out[data.val_mask], y[data.val_mask])\n",
    "        pred = out[data.val_mask].argmax(dim=1)\n",
    "        acc = (pred == y[data.val_mask]).sum() / data.val_mask.sum()\n",
    "    return loss.item(), acc.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test step\n",
    "def test_step(model, data, x, y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x, data.edge_index)\n",
    "        pred = out[data.test_mask].argmax(dim=1)\n",
    "        acc = (pred == y[data.test_mask]).sum() / data.test_mask.sum()\n",
    "    return acc.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "def train_model_semi_supervised(data, num_epochs=250, lr=0.01, hidden_channels=16, alpha=0.1, th=0.5, print_logs=True):\n",
    "    model = GCC(num_features=data.num_features, hidden_channels=hidden_channels, num_classes=data.num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "\n",
    "    x, y = data.x, data.y\n",
    "    # y = get_noisy_labels(data, noise_ratio=0.5)\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_model_path = os.path.join(saved_model_dir, 'best_model_ssl.pth')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        llm_label = True if (epoch % 50) == 0 else False\n",
    "        num_low_conf_samples, total_loss, labeled_loss, consistency_loss, y, data.train_lb_mask, data.train_ulb_mask = train_step_semi_supervised(model, data, optimizer, x, y, alpha, th, llm_label=llm_label)\n",
    "        val_loss, val_acc = validation_step(model, data, x, y)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "        if (epoch % 10 == 0 or epoch == num_epochs) and print_logs:\n",
    "            print(f'Epoch: {epoch:03d},Low Conf Samples: {num_low_conf_samples}')\n",
    "            # print(f'Epoch: {epoch:03d},Low Conf Samples: {num_low_conf_samples}, '\n",
    "            #       f'Total Loss: {total_loss:.4f}, Labeled Loss: {labeled_loss:.4f}, Consistency Loss: {consistency_loss:.4f}, '\n",
    "            #       f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "    model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "    test_acc = test_step(model, data, x, y)\n",
    "    return best_val_acc, test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_supervised(data, num_epochs=200, learning_rate=0.01, hidden_channels=16, print_logs=True):\n",
    "    # Initialize model and optimizer\n",
    "    model = GCC(num_features=data.num_features, hidden_channels=hidden_channels, num_classes=data.num_classes)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Extract features and labels\n",
    "    x = data.x\n",
    "    y = data.y\n",
    "\n",
    "    # Training, validation, and test masks\n",
    "    train_mask = data.train_lb_mask\n",
    "    val_mask = data.val_mask\n",
    "    test_mask = data.test_mask\n",
    "\n",
    "    # Track the best model and accuracy\n",
    "    best_val_acc = 0.0\n",
    "    # best_model_state = None\n",
    "    best_model_path = os.path.join(saved_model_dir, 'best_model_sup.pth')\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass and compute loss\n",
    "        out = model(x, data.edge_index)\n",
    "        # print(f\"out[train_mask] shape: {out[train_mask].shape}\")\n",
    "        # print(f\"y[train_mask] shape: {y[train_mask].shape}\")\n",
    "        # import time\n",
    "        # time.sleep(5)\n",
    "        loss = F.cross_entropy(out[train_mask], y[train_mask])\n",
    "        \n",
    "        # Backward pass and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Validation step\n",
    "        val_loss, val_acc = validation_step(model, data, x, y)\n",
    "\n",
    "        # Update best model if validation accuracy improves\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            # best_model_state = model.state_dict()  # Save best model parameters\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "\n",
    "        # Print training and validation results\n",
    "        if (epoch + 1) % 10 == 0 and print_logs:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Load the best model parameters before testing\n",
    "    model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "    # model.load_state_dict(best_model_state)\n",
    "    test_acc = test_step(model, data, x, y)\n",
    "    # print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    # print(f\"Test Accuracy (Best Model): {test_acc:.4f}\")\n",
    "    \n",
    "    return best_val_acc, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_eval import run_ssl, run_supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrahma56/cs519/SSL_LLM_Node_Classification/TAGLAS/data/dataset.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.data, self.slices = torch.load(self.processed_paths[0])\n",
      "/home/mrahma56/cs519/SSL_LLM_Node_Classification/TAGLAS/data/dataset.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.side_data = torch.load(self.processed_paths[1])\n"
     ]
    }
   ],
   "source": [
    "dataset_key = dataset_key_dict[DATASET]\n",
    "\n",
    "# Experiment with varying unlabeled ratios\n",
    "results = []\n",
    "for ulb_ratio in [0, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]:\n",
    "\n",
    "    #set embedding path to None to use the original features\n",
    "    data_ssl = load_taglas_dataset(dataset_key=dataset_key, unlabel_ratio=ulb_ratio,embedding_path=embedding_path, print_info=False)\n",
    "    \n",
    "    # run these two lines for basic GCC\n",
    "    # _, test_acc_sup = train_model_supervised(data_ssl, print_logs=False)\n",
    "    # _, test_acc_ssl = train_model_semi_supervised(data_ssl, alpha=0.1, th=0.7, print_logs=False)\n",
    "    \n",
    "    #run these two for SOTA GCN\n",
    "    test_acc_sup = run_supervised(data_ssl, device, dataset=DATASET, print_logs=False) \n",
    "    test_acc_ssl = run_ssl(data_ssl, device, dataset=DATASET, print_logs=False)\n",
    "    \n",
    "    \n",
    "    results.append({'ulb_ratio': ulb_ratio, 'supervised_acc': test_acc_sup, 'ssl_acc': test_acc_ssl})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "results_dir = \"/home/mrahma56/cs519/SSL_LLM_Node_Classification/results\"\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(os.path.join(results_dir, f\"results_{DATASET}_{LLM_ID}_{SEED}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabeled Ratio: 0.00, Supervised Acc: 0.7866, SSL Acc: 0.7867\n",
      "Unlabeled Ratio: 0.70, Supervised Acc: 0.7688, SSL Acc: 0.7580\n",
      "Unlabeled Ratio: 0.75, Supervised Acc: 0.7347, SSL Acc: 0.7305\n",
      "Unlabeled Ratio: 0.80, Supervised Acc: 0.7149, SSL Acc: 0.7346\n",
      "Unlabeled Ratio: 0.85, Supervised Acc: 0.7407, SSL Acc: 0.7445\n",
      "Unlabeled Ratio: 0.90, Supervised Acc: 0.7404, SSL Acc: 0.7077\n",
      "Unlabeled Ratio: 0.95, Supervised Acc: 0.6658, SSL Acc: 0.6319\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(f\"Unlabeled Ratio: {r['ulb_ratio']:.2f}, Supervised Acc: {r['supervised_acc']:.4f}, SSL Acc: {r['ssl_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs519",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
